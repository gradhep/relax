{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import relaxed\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey, multivariate_normal\n",
    "import pyhf\n",
    "from typing import Callable, Any\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "Array = jnp.ndarray\n",
    "\n",
    "# specific also\n",
    "def generate_data(\n",
    "    rng=0,\n",
    "    num_points=10000,\n",
    "    sig_mean=(-1, 1),\n",
    "    bup_mean=(4.5, 2),\n",
    "    bdown_mean=(-2.5, -2.5),\n",
    "    b_mean=(0, 0),\n",
    "):\n",
    "    sig = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(sig_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    bkg_up = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(bup_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    bkg_down = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(bdown_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "\n",
    "    bkg_nom = multivariate_normal(\n",
    "        PRNGKey(rng),\n",
    "        jnp.asarray(b_mean),\n",
    "        jnp.asarray([[1, 0], [0, 1]]),\n",
    "        shape=(num_points,),\n",
    "    )\n",
    "    return sig, bkg_nom, bkg_up, bkg_down\n",
    "\n",
    "\n",
    "def hists_from_pars(\n",
    "    pars: dict[str, Array],\n",
    "    data: dict[str, Array],\n",
    "    nn: Callable,\n",
    "    bandwidth: float,\n",
    "    bins: Array | None = None,\n",
    "    scale_factors: dict[str, float] | None = None,\n",
    "    overall_scale: float = 10.0,\n",
    ") -> dict[str, Array]:\n",
    "    \"\"\"Function that takes in data + analysis config parameters, and constructs yields.\"\"\"\n",
    "    nn_output = {k: nn(pars[\"nn_pars\"], data[k]).ravel() for k in data}\n",
    "    make_hist = partial(\n",
    "        relaxed.hist, bandwidth=bandwidth, bins=pars[\"bins\"] if \"bins\" in pars else bins\n",
    "    )\n",
    "    scale_factors = scale_factors or {k: 1.0 for k in nn_output}\n",
    "    # every histogram is scaled to the number of points from that data source in the batch\n",
    "    # so we have more control over the scaling of sig/bkg for realism\n",
    "    hists = {\n",
    "        k: make_hist(nn_output[k]) * scale_factors[k] * overall_scale / len(v)\n",
    "        + 1e-3  # no zeros!\n",
    "        for k, v in nn_output.items()\n",
    "    }\n",
    "    return hists\n",
    "\n",
    "\n",
    "# specific to use case\n",
    "def model_from_hists(hists: dict[str, Array]) -> pyhf.Model:\n",
    "    \"\"\"How to make your HistFactory model from your histograms.\"\"\"\n",
    "    spec = {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"signal\",\n",
    "                        \"data\": hists[\"sig\"],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None},\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": hists[\"bkg_nominal\"],\n",
    "                        \"modifiers\": [\n",
    "                            {\n",
    "                                \"name\": \"correlated_bkg_uncertainty\",\n",
    "                                \"type\": \"histosys\",\n",
    "                                \"data\": {\n",
    "                                    \"hi_data\": hists[\"bkg_up\"],\n",
    "                                    \"lo_data\": hists[\"bkg_down\"],\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    return pyhf.Model(spec, validate=False)\n",
    "\n",
    "\n",
    "def poi_uncert(model: pyhf.Model) -> float:\n",
    "    hypothesis_pars = (\n",
    "        jnp.asarray(model.config.suggested_init()).at[model.config.poi_index].set(1.0)\n",
    "    )\n",
    "    observed_hist = jnp.asarray(model.expected_data(hypothesis_pars))\n",
    "    return relaxed.cramer_rao_uncert(model, hypothesis_pars, observed_hist)[\n",
    "        model.config.poi_index\n",
    "    ]\n",
    "\n",
    "\n",
    "def discovery_significance(model: pyhf.Model, fit_lr: float) -> float:\n",
    "    test_stat = \"q0\"\n",
    "    test_poi = 0.0\n",
    "    hypothesis_pars = (\n",
    "        jnp.asarray(model.config.suggested_init()).at[model.config.poi_index].set(1.0)\n",
    "    )\n",
    "    observed_hist = jnp.asarray(model.expected_data(hypothesis_pars))\n",
    "    return relaxed.infer.hypotest(\n",
    "        test_poi=test_poi,\n",
    "        data=observed_hist,\n",
    "        model=model,\n",
    "        test_stat=test_stat,\n",
    "        expected_pars=hypothesis_pars,\n",
    "        lr=fit_lr,\n",
    "    )\n",
    "\n",
    "\n",
    "def cls_value(model: pyhf.Model, fit_lr: float) -> float:\n",
    "    test_stat = \"q\"\n",
    "    test_poi = 1.0\n",
    "    hypothesis_pars = (\n",
    "        jnp.asarray(model.config.suggested_init()).at[model.config.poi_index].set(0.0)\n",
    "    )\n",
    "    observed_hist = jnp.asarray(model.expected_data(hypothesis_pars))\n",
    "    return relaxed.infer.hypotest(\n",
    "        test_poi=test_poi,\n",
    "        data=observed_hist,\n",
    "        model=model,\n",
    "        test_stat=test_stat,\n",
    "        expected_pars=hypothesis_pars,\n",
    "        lr=fit_lr,\n",
    "    )\n",
    "\n",
    "\n",
    "def generalised_variance(model: pyhf.Model) -> float:\n",
    "    hypothesis_pars = (\n",
    "        jnp.asarray(model.config.suggested_init()).at[model.config.poi_index].set(0.0)\n",
    "    )\n",
    "    observed_hist = jnp.asarray(model.expected_data(hypothesis_pars))\n",
    "    return jnp.linalg.det(\n",
    "        jnp.linalg.inv(relaxed.fisher_info(model, hypothesis_pars, observed_hist))\n",
    "    )\n",
    "\n",
    "\n",
    "def loss_from_model(\n",
    "    model: pyhf.Model,\n",
    "    loss: str | Callable[[dict[str, Any]], float] = \"neos\",\n",
    "    fit_lr=1e-3,\n",
    ") -> float:\n",
    "    if isinstance(loss, Callable):\n",
    "        # everything\n",
    "        return 0\n",
    "    # loss specific\n",
    "    if loss.lower() == \"discovery\":\n",
    "        return discovery_significance(model, fit_lr)\n",
    "    elif loss.lower() in [\"neos\", \"cls\"]:\n",
    "        return cls_value(model, fit_lr)\n",
    "    elif loss.lower() in [\"inferno\", \"poi_uncert\", \"mu_uncert\"]:\n",
    "        return poi_uncert(model)\n",
    "    elif loss.lower() in [\n",
    "        \"general_variance\",\n",
    "        \"generalised_variance\",\n",
    "        \"generalized_variance\",\n",
    "    ]:\n",
    "        return generalised_variance(model)\n",
    "    else:\n",
    "        raise ValueError(f\"loss function {loss} not recognised\")\n",
    "\n",
    "\n",
    "def pipeline(pars, data, bins, nn, loss, bandwidth, keys, scale_factors):\n",
    "    hists = hists_from_pars(\n",
    "        pars=pars,\n",
    "        nn=nn,\n",
    "        data={k: v for k, v in zip(keys, data)},\n",
    "        bandwidth=bandwidth,\n",
    "        bins=bins,\n",
    "        scale_factors=scale_factors,\n",
    "    )\n",
    "    model = model_from_hists(hists)\n",
    "    return loss_from_model(model, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax\n",
    "\n",
    "rng_state = 0  # random state\n",
    "\n",
    "# feel free to modify :)\n",
    "init_random_params, nn = stax.serial(\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1024),\n",
    "    stax.Relu,\n",
    "    stax.Dense(1),\n",
    "    stax.Sigmoid,\n",
    ")\n",
    "\n",
    "num_features = 2\n",
    "_, init_pars = init_random_params(PRNGKey(rng_state), (-1, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as npr\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "data = generate_data(rng=rng_state, num_points=10000)\n",
    "split = train_test_split(*data, random_state=rng_state)\n",
    "train, test = split[::2], split[1::2]\n",
    "\n",
    "num_train = train[0].shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "# batching mechanism\n",
    "def data_stream():\n",
    "    rng = npr.RandomState(rng_state)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "            yield [points[batch_idx] for points in train]\n",
    "\n",
    "\n",
    "batch_iterator = data_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phinate/code/relaxed/venv/lib/python3.9/site-packages/jax/_src/tree_util.py:188: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  warnings.warn('jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() '\n"
     ]
    }
   ],
   "source": [
    "from jaxopt import OptaxSolver\n",
    "import optax\n",
    "\n",
    "\n",
    "bins = jnp.linspace(0, 1, 5)\n",
    "lr = 1e-3\n",
    "num_steps = 100\n",
    "data_types = [\"sig\", \"bkg_nominal\", \"bkg_up\", \"bkg_down\"]\n",
    "loss = partial(\n",
    "    pipeline,\n",
    "    bandwidth=1e-2,\n",
    "    loss=\"neos\",\n",
    "    nn=nn,\n",
    "    keys=data_types,\n",
    "    scale_factors={k: 2.0 if k == \"sig\" else 10.0 for k in data_types},\n",
    ")\n",
    "\n",
    "solver = OptaxSolver(loss, opt=optax.adam(lr), jit=True, maxiter=3)\n",
    "\n",
    "pyhf.set_backend(\"jax\", default=True)\n",
    "state = solver.run_iterator(dict(nn_pars=init_pars), batch_iterator, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.05952097, dtype=float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(dict(nn_pars=init_pars), next(batch_iterator), bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d6333b89854cd01c2018f3ca2f5a59a2cde2765fbca789ff36cfad48ca629b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
